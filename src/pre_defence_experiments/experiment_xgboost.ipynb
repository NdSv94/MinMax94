{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from copy import copy\n",
    "import sys\n",
    "sys.path.append('/home/ndsviriden/MinMax94/src/utils') \n",
    "from constants import data_directory, MmxColumns\n",
    "from interpolation import interpolate_mmx, create_patterns\n",
    "from converters import convert_raw_to_mmx\n",
    "from loaders import load_mm94_stations, select_mm94_features\n",
    "from geographical import find_nearest_wmo_station, add_solar_angles, add_coordinates, add_road_id\n",
    "from sklearn.ensemble import IsolationForest\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# available for test:\n",
    "# 114, 119, 302, 303, 442, 511, 307, 393, 503, 504, 516, 1838, 1896, 117\n",
    "\n",
    "# available and clean:\n",
    "# 20921, 20916, 20761, 20755, 20754, 20743, 20717, 20323\n",
    "# 1921, 1911, 1899, 1831, 1826, 1813, 1808, 704, 702, 635, 628, 626, 615, 593,\n",
    "# 456, 454, 435, 432, 411, 401, 309, 308, 305, 302, 239, 228, 231, 200, 158, 150, \n",
    "# 126, 119, 118, 117, 116, 115, 114, 113\n",
    "\n",
    "# available:\n",
    "# 114, 115, 116, 117, 119, 302, 303, 304, 305, 306, 307,\n",
    "# 308, 309, 393, 442, 470, 471, 472, 502, 503, 504, 505,\n",
    "# 506, 507, 508, 511, 512, 513, 514, 515, 516, 591, 592,\n",
    "# 593, 596, 597, 599, 612, 613, 614, 615, 616, 617, 618,\n",
    "# 619, 620, 621, 622, 623, 624, 625, 626, 627, 628, 629,\n",
    "# 836, 837, 838, 839, 862, 863, 874, 875, 888, 1820, 1832,\n",
    "# 1833, 1834, 1835, 1836, 1838, 1896, 1899, 4007\n",
    "\n",
    "# small outliers:\n",
    "# 114, 119, 302, 303, 442, 511, 512, 514, 629, 838, 839\n",
    "\n",
    "# big bugs\n",
    "# 307, 393, 503, 504, 505, 516, 619, 1838, 1896 -- big bugs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ndsviriden/MinMax94/src/utils/geographical.py:65: RuntimeWarning: invalid value encountered in arccos\n",
      "  solar_azimuth = np.arccos(cos_az) * np.sign(h_rad)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 33.3 s, sys: 3.2 s, total: 36.5 s\n",
      "Wall time: 37.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "test_station_id = [114, 119, 302, 303, 307, 393, 442, 511, 393, 503, 504, 516, 1838, 1896, 117] \n",
    "\n",
    "\n",
    "train_station_id = [20921, 20761, 113, 115, 1899] \n",
    "                   #1921, 1899, 1826, 1808, 702,\n",
    "                    #626, 615, 456, 435, 411,\n",
    "                   #308, 239, 223, 152, 150,\n",
    "                   #126, 118, 116, 115, 113, 20754, 20717]\n",
    "\n",
    "raw = load_mm94_stations(train_station_id + test_station_id)\n",
    "raw = select_mm94_features(raw, ['t_air', 't_road', 't_underroad', 'pressure', 'dampness'])\n",
    "\n",
    "mmx_rwis = convert_raw_to_mmx(raw)\n",
    "mmx_rwis_interpolated = interpolate_mmx(mmx_rwis)\n",
    "data = create_patterns(mmx_rwis_interpolated)\n",
    "\n",
    "data['data_solar_azimuth'], data['data_solar_altitude'] = add_solar_angles(data)\n",
    "data['data_latitude'], data['data_longitude'] = add_coordinates(data)\n",
    "data['data_road'] = add_road_id(data)\n",
    "\n",
    "del data[MmxColumns.ID_AIR_TEMPERATURE], data[MmxColumns.ID_UNDERGROUND_TEMPERATURE], \\\n",
    "    data[MmxColumns.ID_PRESSURE], data[MmxColumns.ID_HUMIDITY]\n",
    "\n",
    "train = data[data['station_id'].isin(train_station_id)]\n",
    "train = train.reset_index(drop=True)\n",
    "\n",
    "test = data[data['station_id'].isin(test_station_id)]\n",
    "test = test.reset_index(drop=True)\n",
    "\n",
    "del data, raw, mmx_rwis, mmx_rwis_interpolated\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add labels to test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_valid_test_ids = []\n",
    "for station_id in test_station_id:\n",
    "    raw = pd.read_csv('/home/ndsviriden/data_valid/' + str(station_id) + '_valid.csv', index_col=False)\n",
    "    loaded_valid_test_ids.append(raw['id'])\n",
    "\n",
    "valid_test_ids = pd.concat(loaded_valid_test_ids)\n",
    "valid_test_ids = list(valid_test_ids.values.ravel())\n",
    "test['label_true'] = (~test[MmxColumns.ID_ROAD_TEMPERATURE].isin(valid_test_ids)).astype(int)\n",
    "# test['label_predicted'] = np.random.binomial(1, 0.15, len(test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_valid_train_ids = []\n",
    "for station_id in train_station_id:\n",
    "    raw = pd.read_csv('/media/ndsviriden/Elements/MinMax94/data/CSV/clean_ids/' + str(station_id) + '_valid.csv', index_col=False, header=None)\n",
    "    loaded_valid_train_ids.append(raw)\n",
    "\n",
    "valid_train_ids = pd.concat(loaded_valid_train_ids)\n",
    "valid_train_ids = list(valid_train_ids.values.ravel())\n",
    "train['clean'] = train[MmxColumns.ID_ROAD_TEMPERATURE].isin(valid_train_ids)\n",
    "train = train[train['clean']]\n",
    "del train['clean']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data(df):\n",
    "    df_res = pd.DataFrame(index=df.index)\n",
    "    df_res['date_time_utc'] = copy(df['date_time_utc'])\n",
    "    df_res['station_id'] = copy(df['station_id'])\n",
    "    for column in [col for col in df.columns if col.startswith('data_')]:\n",
    "        #print(column)\n",
    "        df_res['{0}'.format(column)] = copy(df[column])\n",
    "        if column not in ('data_latitude', 'data_longitude', \n",
    "                         'data_solar_altitude', 'data_solar_azimuth', 'data_road'):\n",
    "            #print(column)\n",
    "            df_res['{0}_lag_1'.format(column)] = df_res['{0}'.format(column)].shift(1) \n",
    "            df_res['{0}_lag_2'.format(column)] = df_res['{0}'.format(column)].shift(2)\n",
    "            df_res['{0}_lag_3'.format(column)] = df_res['{0}'.format(column)].shift(3)\n",
    "            df_res['{0}_lag_4'.format(column)] = df_res['{0}'.format(column)].shift(4)\n",
    "            df_res['{0}_lag_5'.format(column)] = df_res['{0}'.format(column)].shift(5)\n",
    "            #df_res['{0}_lag_6'.format(column)] = df_res['{0}'.format(column)].shift(6)\n",
    "            #df_res['{0}_lag_7'.format(column)] = df_res['{0}'.format(column)].shift(7)\n",
    "            #df_res['{0}_lag_8'.format(column)] = df_res['{0}'.format(column)].shift(8)\n",
    "        \n",
    "            #if column not in ('solar_altitude', 'solar_azimuth'):\n",
    "            #df_res['{0}_diff_12'.format(column)] = df_res['{0}_lag_1'.format(column)] \\\n",
    "            #                                        - df_res['{0}_lag_2'.format(column)]\n",
    "            df_res['{0}_diff_23'.format(column)] = df_res['{0}_lag_2'.format(column)] \\\n",
    "                                                    - df_res['{0}_lag_3'.format(column)]\n",
    "            df_res['{0}_diff_34'.format(column)] = df_res['{0}_lag_3'.format(column)] \\\n",
    "                                                    - df_res['{0}_lag_4'.format(column)]\n",
    "            df_res['{0}_diff_45'.format(column)] = df_res['{0}_lag_4'.format(column)] \\\n",
    "                                                    - df_res['{0}_lag_5'.format(column)]\n",
    "            #df_res['{0}_diff_56'.format(column)] = df_res['{0}_lag_5'.format(column)] \\\n",
    "            #                                       - df_res['{0}_lag_6'.format(column)]\n",
    "            #df_res['{0}_diff_67'.format(column)] = df_res['{0}_lag_6'.format(column)] \\\n",
    "            #                                        - df_res['{0}_lag_7'.format(column)]\n",
    "            #df_res['{0}_diff_78'.format(column)] = df_res['{0}_lag_7'.format(column)] \\\n",
    "            #                                        - df_res['{0}_lag_8'.format(column)]\n",
    "            if column != 'data_t_road':\n",
    "                del df_res['{0}'.format(column)]\n",
    "    \n",
    "    #df_res.set_index('date_time', inplace=True)\n",
    "    df_res['data_dayofyear_cos'] = np.cos(df_res['date_time_utc'].dt.dayofyear / 365 * np.pi)\n",
    "\n",
    "    df_res['data_dayofyear_sin'] = np.sin(df_res['date_time_utc'].dt.dayofyear / 365 * np.pi)\n",
    "\n",
    "    df_res['data_hour_cos'] = np.cos(df_res['date_time_utc'].dt.hour / 24 * np.pi)\n",
    "\n",
    "    df_res['data_hour_sin'] = np.sin(df_res['date_time_utc'].dt.hour / 24 * np.pi)\n",
    "\n",
    "    df_res['data_month_cos'] = np.cos(df_res['date_time_utc'].dt.month / 12 * np.pi)\n",
    "\n",
    "    df_res['data_month_sin'] = np.sin(df_res['date_time_utc'].dt.month / 12 * np.pi)\n",
    "    df_res = df_res.fillna(-10000)\n",
    "    return df_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "point = pd.Timestamp(2015, 6, 1)\n",
    "\n",
    "X_train = create_data(train[train['date_time_utc'] < point])\n",
    "features = [col for col in X_train if col.startswith('data_') and (col!='data_t_road')]\n",
    "X_train = X_train[features].values\n",
    "y_train = train.loc[train['date_time_utc'] < point, 'data_t_road'].values.reshape(-1, 1)\n",
    "\n",
    "X_valid = create_data(train[train['date_time_utc'] >= point])\n",
    "X_valid = X_valid[features].values\n",
    "y_valid = train.loc[train['date_time_utc'] >= point, 'data_t_road'].values.reshape(-1, 1)\n",
    "\n",
    "X_test = create_data(test)\n",
    "X_test = X_test[features].values\n",
    "y_test = test['data_t_road'].values.reshape(-1, 1)\n",
    "\n",
    "\n",
    "dtrain = xgb.DMatrix(X_train, y_train)\n",
    "dval = xgb.DMatrix(X_valid, y_valid)\n",
    "dtest = xgb.DMatrix(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------MEDIAN-----------\n",
      "[0]\ttrain-mae:nan\tvalid-mae:nan\n",
      "[20]\ttrain-mae:nan\tvalid-mae:nan\n",
      "[40]\ttrain-mae:nan\tvalid-mae:nan\n",
      "[60]\ttrain-mae:nan\tvalid-mae:nan\n",
      "[80]\ttrain-mae:nan\tvalid-mae:nan\n",
      "[100]\ttrain-mae:nan\tvalid-mae:nan\n",
      "[120]\ttrain-mae:nan\tvalid-mae:nan\n",
      "[140]\ttrain-mae:nan\tvalid-mae:nan\n",
      "[160]\ttrain-mae:nan\tvalid-mae:nan\n",
      "[180]\ttrain-mae:nan\tvalid-mae:nan\n",
      "[199]\ttrain-mae:nan\tvalid-mae:nan\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error as mse\n",
    "import xgboost as xgb\n",
    "from functools import partial\n",
    "\n",
    "params = {}\n",
    "\n",
    "params[\"eval_metric\"] = 'mae'\n",
    "params[\"eta\"] = 0.05\n",
    "params[\"lambda\"] = 0.3\n",
    "params[\"subsample\"] = 0.8 #1\n",
    "params[\"min_child_weight\"] = 1.\n",
    "params[\"colsample_bytree\"] = 0.5\n",
    "params[\"max_depth\"] = 9\n",
    "params[\"silent\"] = 1\n",
    "params[\"gamma\"] = 0.1\n",
    "delta = 1.\n",
    "#delta = 1.\n",
    "watchlist = [(dtrain, 'train'), (dval, 'valid')]\n",
    "\n",
    "#-----------------------------------------------------------------------------------------------\n",
    "print('-----------MEDIAN-----------')\n",
    "clf_dirty = xgb.train(params, dtrain, 200, watchlist, verbose_eval=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = dtest.get_label()\n",
    "y_pred = clf_dirty.predict(dtest).ravel()\n",
    "upper = y_pred + 0.48 * 10\n",
    "lower = y_pred - 0.48 * 10\n",
    "test['label_predicted'] = ((y_true > upper) | (y_true < lower))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(test[test['label_predicted']==1]) / len(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate Precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_anomalies_ids = set()\n",
    "window = pd.Timedelta('3h')\n",
    "\n",
    "st_id = [114, 119, 302, 303, 442, 511, 504, 516, 1838, 1896, 117]\n",
    "for station in st_id:\n",
    "    df = test[test['station_id']==station]\n",
    "    true_anomalies = df[df['label_true']==1]\n",
    "    for anomaly in true_anomalies.iterrows():\n",
    "        dt = anomaly[1]['date_time_utc']\n",
    "        locality = test[(test['date_time_utc'] >= (dt - window)) & \n",
    "                       (test['date_time_utc'] <= (dt + window))]\n",
    "        \n",
    "        true_anomalies_ids.update(set(locality.index))\n",
    "        # print(list(locality.index))\n",
    "\n",
    "predicted_anomalies_ids = set(test[((test['label_predicted']==1) & (test['station_id'].isin(st_id)))].index)\n",
    "\n",
    "tp = set.intersection(true_anomalies_ids, predicted_anomalies_ids)\n",
    "precision = len(tp) / len(predicted_anomalies_ids)\n",
    "precision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_anomalies_ids = set()\n",
    "window = pd.Timedelta('3h')\n",
    "\n",
    "# [114, 119, 302, 303, 442, 511, ?307, ?393, ?503, 504, 516, 1838, 1896, 117]\n",
    "# for station in test.station_id.unique():\n",
    "st_id = [114, 119, 302, 303, 442, 511, 504, 516, 1838, 1896, 117]\n",
    "for station in st_id:\n",
    "    df = test[test['station_id']==station]\n",
    "    predicted_anomalies = df[df['label_predicted']==1]\n",
    "    for anomaly in predicted_anomalies.iterrows():\n",
    "        dt = anomaly[1]['date_time_utc']\n",
    "        locality = test[(test['date_time_utc'] >= (dt - window)) & \n",
    "                       (test['date_time_utc'] <= (dt + window))]\n",
    "        \n",
    "        predicted_anomalies_ids.update(set(locality.index))\n",
    "        # print(list(locality.index))\n",
    "\n",
    "true_anomalies_ids = set(test[(test['label_true']==1) & (test['station_id'].isin(st_id))].index)\n",
    "\n",
    "tp = set.intersection(true_anomalies_ids, predicted_anomalies_ids)\n",
    "recall = len(tp) / len(true_anomalies_ids)\n",
    "recall, len(true_anomalies_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# test_station_id = [114, 119, 302, 303, 307, 393, 442, 511, 393, 503, 504, 516, 1838, 1896, 117] \n",
    "\n",
    "# 307, 393, 503, 504, 505, 516, 619, 1838, 1896 -- big bugs\n",
    "\n",
    "df_station = copy(train[train['station_id']==1899].set_index('date_time_utc'))\n",
    "#df_station = copy(train[train['station_id']==113].set_index('date_time_utc'))\n",
    "\n",
    "start = pd.Timestamp(2012, 1, 1)\n",
    "end = pd.Timestamp(2014, 2, 1)\n",
    "\n",
    "to_plot = df_station[(df_station.index<=end) & (df_station.index>=start)]\n",
    "\n",
    "plt.figure(figsize=(40, 10))\n",
    "\n",
    "#cond = (to_plot['label_true']) == 1\n",
    "#plt.plot_date(to_plot[cond].index, to_plot[cond]['data_t_road'], 'g',\n",
    "#                      linestyle='none', marker='o', markersize=12, label='t_road')\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"for elem in [True, False]:\n",
    "    cond = (to_plot['label_predicted']) != elem\n",
    "    if elem:\n",
    "        i=1\n",
    "        plt.plot_date(to_plot[cond].index, to_plot[cond]['data_t_road'], 'b',\n",
    "                      linestyle='none', marker='o', markersize=8, label='t_road')\n",
    "    else:\n",
    "        plt.plot_date(to_plot[cond].index, to_plot[cond]['data_t_road'], 'r.',\n",
    "                              linestyle='none', marker='o', label='outliers', markersize=8)\"\"\"\n",
    "\n",
    "plt.plot_date(to_plot.index, to_plot['data_t_road'], 'b',\n",
    "                      linestyle='-', marker='None', markersize=8, label='t_road', linewidth=3)\n",
    "\n",
    "plt.grid()\n",
    "plt.legend(fontsize=40)\n",
    "plt.ylabel(r' Road temperature, $^{\\circ}C$', fontsize=40)\n",
    "\n",
    "plt.tick_params(labelsize=40)\n",
    "#plt.show()\n",
    "plt.savefig('/home/ndsviriden/PycharmProjects/MinMax94/src/pre_defence_plots/typical_large')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
